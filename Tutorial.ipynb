{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SIENA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain how to use SIENA we will use the __Islam__ dataset provided in ./data. The steps are all the same for any single cell dataset, however the parameterization in each step can very depending on what we want to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SIENA.siena import *\n",
    "data = CSVDataset(\"./data/Islam\", \"Islam_treated.csv\", labels_file=\"Islam_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential expression analysis (4 steps):\n",
    "### 1 - Create a model instance.\n",
    "This is the step where we pass the counts, type and batch information (cell types, type of each cell, batch names and batch of each cell) and it is where we construct the model. To do so, we can specify if we want (or not) zero-inflation, gene-dispersion, batch-training (if so how many cells per mini-batch) and which prior to posit over the library scalings ($L_i$)\n",
    "\n",
    "### 2 - Inference.\n",
    "In this step, posterior approximations are generated for variables $\\beta_{ig}$ and $L_i$, and $\\theta_g$ are estimated. The optimization process takes $N$ number of iterations which can be specified.\n",
    "\n",
    "As a first example, we'll create a model with gene-dispersion and log-normal library scalings, but with no zero-inflation and no batch-training during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using log scalings\n",
      "Minisamples: 1\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 106s | Loss: 17642.791\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "model = ModelDE(data.X,data.cell_types,data.labels, zero_inflation=False, dispersion=True, use_log=True)\n",
    "model.approximate_model(iters=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a __Gamma__ prior over the library factors instead of a log-normal, we simply need to set false the use_log parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minisamples: 1\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 107s | Loss: 17646.026\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "model = ModelDE(data.X,data.cell_types,data.labels, zero_inflation=False, dispersion=True, use_log=False)\n",
    "model.approximate_model(iters=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use __mini-batches__ during inference, we only have to specify the number of cells per mini-batch, i.e., set minisample parameter. If minisample is None (default), SIENA uses the whole dataset in each update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using log scalings\n",
      "Minisamples: 2\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 127s | Loss: 17572.737\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "model = ModelDE(data.X,data.cell_types,data.labels, zero_inflation=False, dispersion=True, use_log=True, minisample=46)\n",
    "model.approximate_model(iters=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset contains __BATCH information__, we need to pass it in the model constructor. __Note that the batch information is only necessary for the DE tests__. To exemplify we'll <font color=blue>simulate a batch assignement</font> for the Islam dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Using log scalings\n",
      "Minisamples: 1\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 107s | Loss: 17659.534\n"
     ]
    }
   ],
   "source": [
    "_batches = np.array([\"batch 1\", \"batch 2\"])\n",
    "_batch_idx = np.zeros((92,))\n",
    "_batch_idx[24:48] = 1\n",
    "_batch_idx[70:92] = 1\n",
    "print(_batch_idx)\n",
    "\n",
    "N = 1000\n",
    "model = ModelDE(data.X,data.cell_types,data.labels, batches=_batches, batch_idx=_batch_idx, \n",
    "                zero_inflation=False, dispersion=True, use_log=True)\n",
    "model.approximate_model(iters=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Calculate the differential scores for each gene.\n",
    "In this step, average Bayes factors are calculated for each gene, using all or a subset of valid cell pairs and a given number of Monte Carlo samples for each $\\rho_{ig}$. If the model object was created with batch assignments, the DE scores are computed taking into account such information. See the preprint for more details.\n",
    "\n",
    "In the following example we use __ALL pairs__ and 100 MC samples for each $\\rho_{ig}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho1_values shape: (100, 44, 6757)\n",
      "rho2_values shape: (100, 48, 6757)\n",
      "[ 1.48844605 -1.43643149  0.45411687 ...  0.04989049  0.08537334\n",
      " -0.66893374]\n"
     ]
    }
   ],
   "source": [
    "scores = model.differential_expression_scores(\"MEF\",\"ESC\", n_samples=100)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to use a __SUBSET of pairs__, then we need to specify the number of pairs to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho1_values shape: (100, 44, 6757)\n",
      "rho2_values shape: (100, 48, 6757)\n",
      "[ 1.56169842 -1.73404099  0.44343199 ...  0.03968778  0.08409054\n",
      " -0.69380853]\n"
     ]
    }
   ],
   "source": [
    "scores = model.differential_expression_scores(\"MEF\",\"ESC\", n_samples=100, n_pairs=500)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Rank and select (optional) genes based on the DE scores.\n",
    "To obtain the __RANKING with all genes__, we simply pass the gene identifiers and the scores. Note that the ids and scores must be in the same order. If the CSVDataset object is used then that is already fulfilled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Gene    factor\n",
      "6583    Bex1  9.922237\n",
      "2547  Pou5f1  9.685674\n",
      "3597   Thbs1  8.850172\n",
      "6555    Bex4  8.517200\n",
      "5814  Cyp2e1  8.439323\n"
     ]
    }
   ],
   "source": [
    "rank = model.differentially_expressed_genes(data.gene_names, scores)\n",
    "print(rank.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to chose the __DEG__, a treshold must be specified (~ 2 or 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Gene    factor\n",
      "6583    Bex1  9.922237\n",
      "2547  Pou5f1  9.685674\n",
      "3597   Thbs1  8.850172\n",
      "6555    Bex4  8.517200\n",
      "5814  Cyp2e1  8.439323\n"
     ]
    }
   ],
   "source": [
    "DEG = model.differentially_expressed_genes(data.gene_names, scores, threshold=2)\n",
    "print(DEG.head(n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess multiple AUC\n",
    "\n",
    "To assess the __mean AUC__ of SIENA, we need to repeat the described pipeline multiple times and average the AUC obtained in each run. This is the same process as the one used in the paper to calculate SIENA's AUC means.\n",
    "Here we exemplify the procedure for the __Islam__ dataset using only 2 runs.\n",
    "\n",
    "First let us define a function to calculate the AUC of each run, using the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def calculate_auc(scrs, lbls, trueDEG):\n",
    "    scores = np.array(scrs)\n",
    "    labels = np.array(lbls.isin(trueDEG))\n",
    "    fpr,tpr,thresh = roc_curve(labels, scores)\n",
    "    return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perfrom and calculate the AUC of the 2 runs, as well as the mean AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using log scalings\n",
      "Minisamples: 1\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 107s | Loss: 17626.478\n",
      "rho1_values shape: (100, 44, 6757)\n",
      "rho2_values shape: (100, 48, 6757)\n",
      "Using log scalings\n",
      "Minisamples: 1\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 107s | Loss: 17621.376\n",
      "rho1_values shape: (100, 44, 6757)\n",
      "rho2_values shape: (100, 48, 6757)\n",
      "AUCs: [0.6891779283676716, 0.6919780843643835]\n",
      "Average AUC: 0.6905780063660275\n"
     ]
    }
   ],
   "source": [
    "n_times = 2 #number of runs\n",
    "n_epochs = 1000\n",
    "path = './data/Islam'\n",
    "trueDEG = pd.read_csv(path + '/trueDEG.txt', sep=\",\", header=0)\n",
    "data = CSVDataset(path, \"Islam_treated.csv\", labels_file=\"Islam_labels.csv\")\n",
    "aucs = []\n",
    "for i in range(n_times):\n",
    "    model = ModelDE(data.X,data.cell_types,data.labels, zero_inflation=False, dispersion=True, use_log=True)\n",
    "    model.approximate_model(iters=n_epochs)\n",
    "    scores = model.differential_expression_scores(\"MEF\",\"ESC\", n_samples=100)\n",
    "    res = model.differentially_expressed_genes(data.gene_names,scores)\n",
    "    auc_myModel = calculate_auc(res['factor'], res[\"Gene\"], trueDEG[\"SYMBOL\"])\n",
    "    aucs.append(auc_myModel)\n",
    "print('AUCs: ' + str(aucs))\n",
    "print('Average AUC: ' + str(np.mean(np.array(aucs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the __synthetic__ data the procedure is almost the same, with some small changes.\n",
    "Here we exemplify the procedure with the 50-50-50-50 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using log scalings\n",
      "No Dispersion\n",
      "Minisamples: 1\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 82s | Loss: 24688.642\n",
      "rho1_values shape: (100, 500, 1000)\n",
      "rho2_values shape: (100, 500, 1000)\n",
      "Using log scalings\n",
      "No Dispersion\n",
      "Minisamples: 1\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 82s | Loss: 24241.558\n",
      "rho1_values shape: (100, 500, 1000)\n",
      "rho2_values shape: (100, 500, 1000)\n",
      "[0.92149375, 0.9137875]\n",
      "Average AUC: 0.917640625\n"
     ]
    }
   ],
   "source": [
    "n_times = 2 #number of runs\n",
    "n_epochs = 1000\n",
    "path = './data/Synthetic/50-50-50-50-400-400'\n",
    "trueDEG = pd.read_csv(path + '/trueDEG.txt', sep=\",\", header=0)\n",
    "data = CSVDataset(path, \"synthetic.csv\", gene_by_cell=True, labels_file=\"labels.csv\")\n",
    "aucs = []\n",
    "for i in range(n_times):\n",
    "    model = ModelDE(data.X,data.cell_types,data.labels, zero_inflation=False, dispersion=False, use_log=True)\n",
    "    model.approximate_model(iters=n_epochs)\n",
    "    scores = model.differential_expression_scores(\"B\",\"A\", n_samples=100)\n",
    "    res = model.differentially_expressed_genes(data.gene_names,scores)\n",
    "    auc_myModel = calculate_auc(res['factor'], res[\"Gene\"], trueDEG[\"Gene\"])\n",
    "    aucs.append(auc_myModel)\n",
    "print(aucs)\n",
    "print('Average AUC: ' + str(np.mean(np.array(aucs))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
